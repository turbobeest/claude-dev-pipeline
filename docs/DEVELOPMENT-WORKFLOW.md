# TaskMaster + OpenSpec Development Workflow
## Systematic Software Development for Bug-Free Code

**Version:** 2.0  
**Last Updated:** October 2025  
**Usage Phases:** 0 (Setup) through 6 (Deployment) - Complete Reference Guide

---

## Prerequisites

**HUMAN OPERATOR must complete BEFORE starting Claude Code:**

### Phase 0: Setup & Planning

#### Step 1: Create PRD Using Template
- [x] PRD document created using `PRD_Template_for_TaskMaster-Driven_Development`
- [x] PRD includes **Section 4: Integration & System Testing Requirements**
- [x] Solution architecture documented in `docs/architecture.md`
- [x] Architecture diagram available
- [x] Save PRD to `.taskmaster/docs/prd.txt`

#### Step 2: Generate tasks.json Using Claude Projects
- [x] Open Claude Projects interface
- [x] Paste prompt from `Claude_Projects_Prompt__Generate_Production-Grade_tasks.json`
- [x] Provide: PRD + Architecture Diagram  
- [x] Claude Projects generates complete `tasks.json` with:
  - 15-25 top-level tasks
  - **NO subtasks** (will be generated by TaskMaster in Phase 1)
  - Integration & validation tasks (typically #24-26) included
  - Dependencies mapped correctly
  - OpenSpec mappings defined per task
  - Complete acceptance criteria per task
- [x] Save generated `tasks.json` to `.taskmaster/tasks.json`

#### Step 3: Human Review & Validation (CRITICAL - DO NOT SKIP)

**Purpose:** Ensure quality and completeness before automation takes over.

**Review Checklist:**
- [x] **Task Count**: Verify 15-25 top-level tasks generated
- [x] **PRD Coverage**: Confirm ALL PRD requirements mapped to tasks
- [x] **Integration Tasks Present**: Verify dedicated tasks exist for:
  - Component Integration Testing (tests all integration points)
  - End-to-End Workflow Testing (tests complete user journeys)
  - Production Readiness Validation (completes checklist, obtains sign-offs)
- [x] **Dependency Validation**: Check for:
  - No circular dependencies
  - Logical dependency chains
  - Integration tasks depend on ALL feature tasks
- [x] **Architecture Alignment**: Confirm tasks map to architecture components
- [x] **Test Strategy**: Verify each task has comprehensive testStrategy field

**Generate 1-Page Task Summary:**

Create summary document containing:

```markdown
# Task Summary for [Project Name]

## Overview
- Total Tasks: [X]
- Foundation Tasks: [count] (tasks 1-Y)
- Feature Tasks: [count] (tasks Y+1-Z)
- Integration Tasks: [count] (tasks Z+1-end)

## Task Distribution by Category
- Data Layer: [X] tasks
- Business Logic: [Y] tasks
- API Layer: [Z] tasks
- Frontend: [A] tasks (if applicable)
- Testing Infrastructure: [B] tasks
- Documentation: [C] tasks
- Integration: [D] tasks (CRITICAL)

## Critical Path Analysis
1. [Task #X] -> Blocks: [list]
2. [Task #Y] -> Blocks: [list]
...

## Dependency Graph
Foundation Phase: [Task #1] -> [Task #2] -> [Task #3]
Feature Phase (Parallel): 
  Branch A: [Task #4] -> [Task #7]
  Branch B: [Task #5] -> [Task #8]
Integration Phase (Sequential):
  [Task #24] -> [Task #25] -> [Task #26]

## Integration Strategy Confirmation
Task #24: Component Integration Testing
Task #25: End-to-End Workflow Testing
Task #26: Production Readiness Validation

## Issues Found & Resolved
[List any gaps/inconsistencies found during review]

## Approval
Reviewed by: [Name]
Date: [Date]
Status: APPROVED
```

**Adjudicate Issues:**
- Fix missing tasks
- Resolve inconsistencies
- Add clarification to ambiguous descriptions
- Ensure integration tasks have correct dependencies

**Final Sign-Off:**
- [x] tasks.json validated and ready for TaskMaster
- [x] 1-page summary generated and approved
- [x] All stakeholders aligned on task structure

**Output:**
- Validated `.taskmaster/tasks.json`
- 1-page task summary document
- Confidence to proceed with automated complexity analysis


#### Step 3.5: Checkpoint Verification (ENFORCED GATE)

**Purpose:** Automated verification that all Phase 0 validations are complete before tools initialization.

**Action:** Run the checkpoint script:
```bash
bash .taskmaster/scripts/phase0-checkpoint.sh
```

**This script validates:**
1. ✅ `tasks.json` exists and is valid JSON
2. ✅ `tasks.json` committed to git  
3. ✅ Sign-off document created (`.taskmaster/docs/phase0-signoff.md`)
4. ✅ Task count in reasonable range (15-25)
5. ✅ Integration tasks present
6. ✅ 1-page summary document exists

**Possible Outcomes:**

**✅ CHECKPOINT PASSED:**
```bash
✅ CHECKPOINT PASSED

You may proceed to Phase 0 Step 5 (Tool Initialization)
```
→ **Proceed to Step 4** (Tool Initialization)

**❌ CHECKPOINT FAILED:**
```bash
❌ CHECKPOINT FAILED

⛔ CANNOT PROCEED TO TOOL INITIALIZATION
[List of specific failures]
```
→ **BLOCKED** - Fix listed issues and re-run checkpoint  
→ **DO NOT proceed to Step 4** until checkpoint passes

**Why This Gate Exists:**

Without this gate, automation in Phase 1-3 could proceed with:
- Invalid or uncommitted task definitions → Lost work
- Missing integration tasks → Incomplete test coverage  
- No documentation → No audit trail
- Unresolved validation issues → Cascading failures

This 30-second checkpoint prevents hours of wasted automation effort.

---

#### Step 4: Initialize Tools
- [x] TaskMaster initialized: `task-master init`
- [x] OpenSpec initialized: `openspec init --tools claude`
- [x] Create `TASKMASTER_OPENSPEC_MAP.md` in project root (will be populated during workflow)

#### Step 5: Configure Testing Infrastructure
- [x] **Testing framework configured** (Jest, Pytest, etc.)
- [x] **Test coverage tool installed** (Istanbul, Coverage.py, etc.)
- [x] **CI/CD pipeline configured** (optional but recommended)
- [x] **Minimum coverage thresholds defined** (default: 80% line, 70% branch)
- [x] **Test types established**: Unit, Integration, E2E

**Claude Code starts with:**
- `.taskmaster/tasks.json` exists (generated by Claude Projects, validated by human)
- 1-page task summary document for reference
- Initial task list (15-25 top-level tasks) ready for complexity analysis
- **NO subtasks yet** - TaskMaster will generate these in Phase 1

---

## Workflow Overview

```
Phase 0: Setup & Planning (Human)
    - PRD Generation
    - Task Generation (Claude Projects)
    - Human Review & Validation (CRITICAL)
    |
Phase 1: Task Decomposition (Claude Code + TaskMaster)
    - Complexity Analysis (5-15 min)
    - Subtask Generation (research-backed)
    |
Phase 2: Specification Generation (Claude Code + OpenSpec)
    - Batched processing (10-20 min per batch)
    - Coupling analysis
    |
Phase 3: Implementation (Claude Code + TDD)
    - Sequential or parallel
    - RED-GREEN-REFACTOR cycle
    |
Phase 4: Component Integration (Claude Code)
    - Task #24: Integration Testing
    |
Phase 5: E2E & Production Validation (Claude Code)
    - Task #25: E2E Workflows
    - Task #26: Production Readiness
    |
Phase 6: Deployment
    - Staging -> Canary -> Production
```

**Key Principle:** This workflow ensures bug-free code through:
1. Purpose-built PRD with integration requirements (not afterthought)
2. **Human-validated task structure before automation**
3. Complete task decomposition with research-backed subtasks
4. Explicit specifications before implementation
5. **Test-Driven Development (TDD) - tests written BEFORE implementation**
6. **Mandatory test coverage gates - no merge without passing tests**
7. **Explicit integration and system validation phases**
8. Continuous validation and traceability

---

## Phase 1: Task Decomposition (CLAUDE CODE)

**Objective:** Use TaskMaster to analyze complexity of human-validated tasks and generate research-backed subtasks for high-complexity items.

### Starting Point

**When Claude Code begins Phase 1:**
- [x] `.taskmaster/tasks.json` exists (generated by Claude Projects, reviewed by human)
- [x] 15-25 top-level tasks with NO subtasks
- [x] Each task has complete acceptance criteria, test strategy, dependencies
- [x] Integration tasks included (e.g., #24-26)
- [ ] NO complexity analysis yet (TaskMaster will do this)
- [ ] NO subtasks yet (TaskMaster will generate for high-complexity tasks)

### Why This Approach?

**Claude Projects Role (Strategic):**
- Understand PRD holistically
- Map requirements to architecture
- Create top-level task structure
- Define acceptance criteria
- Set dependencies

**TaskMaster Role (Tactical):**
- Analyze complexity per task
- Research best practices (--research flag)
- Generate implementation subtasks
- Break down high-complexity work
- Estimate effort

**Separation Prevents:**
- Context window bloat (Claude Projects: 15K tokens vs. upfront subtasks: 25K+ tokens)
- Rate limit issues (TaskMaster makes small, independent API calls)
- Loss of research integration (TaskMaster actively searches during expansion)
- Inability to iterate (can re-expand if subtasks wrong)

### Phase 1 Commands for Claude Code

```bash
# 1. Verify tasks exist and review structure
task-master list

# Expected output: 15-25 tasks, NO subtasks yet
# Example:
# Tasks:
# 1. Set up development environment
# 2. Design database schema
# 3. Implement user authentication
# ...
# 24. Component integration testing
# 25. End-to-end workflow testing
# 26. Production readiness validation

# 2. Analyze complexity for ALL tasks (research-backed)
task-master analyze-complexity --research

# This makes ONE API call per task (e.g., 25 separate calls)
# Each call: ~2.5K tokens (small context)
# Total: 25 x 2.5K = ~62.5K across all calls (NOT in one call)
# NO rate limit concerns - calls are sequential and independent

# 3. Generate complexity report
task-master complexity-report

# Example output:
# Complexity Analysis Report
# ==========================
# Total tasks: 25
# 
# High complexity (>= 7): 8 tasks
#   - Task #3: Implement user authentication (complexity: 8)
#   - Task #5: Build payment processing (complexity: 9)
#   - Task #7: Create admin dashboard (complexity: 8)
#   - Task #10: Implement notification system (complexity: 7)
#   - Task #12: Build reporting engine (complexity: 9)
#   - Task #15: Create data export system (complexity: 7)
#   - Task #18: Implement real-time updates (complexity: 8)
#   - Task #21: Build analytics dashboard (complexity: 7)
#
# Medium complexity (4-6): 12 tasks
# Low complexity (1-3): 5 tasks

# 4. Expand ONLY high-complexity tasks (complexity >= 7)
# For each task identified in complexity report:

task-master expand --id=3 --research
# Behind the scenes:
# - Reads task description
# - Researches: "Best practices user authentication 2025"
# - Researches: "JWT token security"
# - Researches: "Password hashing bcrypt vs argon2"
# - Generates 4-8 subtasks based on research
# - Each call: ~5.5K tokens
# Total for 8 expansions: 8 x 5.5K = ~44K across all calls

task-master expand --id=5 --research
task-master expand --id=7 --research
task-master expand --id=10 --research
task-master expand --id=12 --research
task-master expand --id=15 --research
task-master expand --id=18 --research
task-master expand --id=21 --research

# 5. Review final structure
task-master list --compact

# Example output:
# Tasks: 25 total
# Subtasks: 38 total (from 8 expanded tasks)
# Task #3: User authentication [4 subtasks]
# Task #5: Payment processing [5 subtasks]
# Task #7: Admin dashboard [5 subtasks]
# Task #10: Notification system [4 subtasks]
# Task #12: Reporting engine [6 subtasks]
# Task #15: Data export [4 subtasks]
# Task #18: Real-time updates [5 subtasks]
# Task #21: Analytics dashboard [5 subtasks]

# 6. Verify specific expanded tasks
task-master show 3

# Shows:
# Task #3: Implement User Authentication
#   3.1: Create user data model with password hashing
#   3.2: Implement registration endpoint with validation
#   3.3: Implement login endpoint with JWT generation
#   3.4: Create JWT validation middleware
#   [Each subtask has description, acceptance criteria, test strategy]
```

### Phase 1 Deliverables

After Phase 1, provide to stakeholders:

**Task Structure Summary:**
```
Total Tasks: 25
Total Subtasks: 38 (from 8 high-complexity tasks)

High-Complexity Tasks Expanded:
- Task #3: User Authentication (4 subtasks)
- Task #5: Payment Processing (5 subtasks)
- Task #7: Admin Dashboard (5 subtasks)
- Task #10: Notification System (4 subtasks)
- Task #12: Reporting Engine (6 subtasks)
- Task #15: Data Export (4 subtasks)
- Task #18: Real-time Updates (5 subtasks)
- Task #21: Analytics Dashboard (5 subtasks)

Low/Medium Complexity (No Expansion Needed):
- 17 tasks will be implemented directly (no subtasks)

Dependency Analysis:
- Critical Path: Tasks #1 -> #2 -> #3 -> #24 -> #25 -> #26
- Parallel Opportunities: Tasks #4-8 can run simultaneously
- Integration Dependency: Task #24 depends on tasks #1-23

Batching Strategy for Phase 2:
Batch 1: Tasks 1-5 (foundation + core auth)
Batch 2: Tasks 6-10 (core features)
Batch 3: Tasks 11-15 (secondary features)
Batch 4: Tasks 16-20 (advanced features)
Batch 5: Tasks 21-23 (remaining features)
Batch 6: Tasks 24-26 (integration & validation)
```

### Why Not Generate Subtasks Upfront in Claude Projects?

**Rate Limit Concerns** (Your Original Question):
- **Claude Projects approach:** One massive prompt (PRD + all tasks + subtask generation) = 25K+ tokens in single call
- **TaskMaster approach:** 33 small calls (25 analyze + 8 expand) = ~2.5K-5.5K per call, NO cumulative context

**Research Integration:**
- **Claude Projects:** Static knowledge cutoff (January 2025)
- **TaskMaster --research:** Actively searches web for current best practices during expansion

**Iterative Refinement:**
- **Claude Projects:** One shot, if wrong must regenerate entire tasks.json
- **TaskMaster:** Can re-expand individual tasks, human can edit JSON directly

**Complexity-Aware:**
- **Claude Projects:** Must guess which tasks need subtasks (inconsistent)
- **TaskMaster:** Scientifically analyzes complexity, expands only high-complexity tasks

---

## Phase 2: Specification Generation (CLAUDE CODE)

**Objective:** Create OpenSpec proposals for tasks/subtasks based on coupling analysis. Process in logical batches to maintain context quality.

### Batching Strategy

**Based on total task count:**
- **Small projects (<10 tasks):** Process all at once
- **Medium projects (10-30 tasks):** Process in 2-3 batches of 5-10 tasks
- **Large projects (30+ tasks):** Process in batches of 5-10 tasks, prioritize by dependencies

**Typical batch size:** 5-10 tasks  
**Process sequentially:** Complete each batch before starting next

### Decision Framework: Task-Level vs Subtask-Level Proposals

**For EACH task, analyze subtask coupling:**

```
IF task has NO subtasks OR <= 2 subtasks:
    -> Strategy: ONE PROPOSAL per task
    
ELSE IF subtasks are TIGHTLY COUPLED:
    (share code, models, utilities - must implement together)
    -> Strategy: ONE PROPOSAL per task
    -> Map each subtask to a requirement section
    
ELSE IF subtasks are LOOSELY COUPLED:
    (independent files, can parallelize)
    -> Strategy: ONE PROPOSAL per subtask
    -> Enables parallel implementation
```

**Coupling Indicators:**

**Tightly Coupled:**
- Share same classes/models
- Modify same files
- Depend on each other's outputs
- Example: User model + all auth endpoints

**Loosely Coupled:**
- Touch different files completely
- No shared dependencies
- Can test independently
- Example: Different API endpoint refactors

### Commands for Each Batch

```bash
# Step 1: Analyze tasks in current batch
task-master show <id>  # For each task in batch

# Document coupling analysis:
# Task #X: [Title]
# - Subtasks: [count]
# - Coupling: [TIGHTLY COUPLED / LOOSELY COUPLED / NO SUBTASKS]
# - Strategy: [One proposal / Multiple proposals]
# - Rationale: [Why?]

# Step 2: Create proposals based on strategy
```

### Approach A: Tightly Coupled (One Proposal)

Use this for: Tasks with shared code or no subtasks

```bash
/openspec:proposal <descriptive-name>
```

**Proposal Structure:**
```markdown
# [Descriptive Name]

## Motivation
[Extract from TaskMaster task description and business context]

## Scope
Related TaskMaster Task: #[X] (subtasks [X.1]-[X.N])

## Requirements

### Requirement: [Subtask 1 Intent] (TM [X.1])
The system SHALL [specific behavior]...

**Scenarios:**
- GIVEN [precondition]
  WHEN [action]
  THEN [expected result]

[Map from TaskMaster acceptance criteria]

**Test Coverage Requirements:**
- Unit tests: [List specific test cases]
- Integration tests: [List integration points]
- Edge cases: [List boundary conditions]
- Error cases: [List failure scenarios]
- Minimum coverage: 80% line, 70% branch

### Requirement: [Subtask 2 Intent] (TM [X.2])
[Continue for each subtask...]

## Test Strategy
**MANDATORY TEST-DRIVEN APPROACH:**

1. **Pre-implementation:**
   - Write failing tests for ALL scenarios FIRST
   - Define test fixtures and mocks
   - Document expected behavior in test assertions
   
2. **Test Types Required:**
   - Unit tests: Test each function/method in isolation
   - Integration tests: Test component interactions
   - E2E tests: Test complete user workflows (if applicable)
   - Regression tests: Ensure no existing functionality breaks

3. **Coverage Thresholds (BLOCKING):**
   - Minimum 80% line coverage
   - Minimum 70% branch coverage
   - 100% coverage of happy paths
   - 100% coverage of error handling

4. **Test Execution Gates:**
   - All tests MUST pass before marking subtask complete
   - Coverage thresholds MUST be met
   - No warnings or linting errors
   - Integration tests MUST pass on clean state

5. **Continuous Validation:**
   - Run tests after each code change
   - Monitor coverage changes
   - Verify no regression in existing tests
```

**Validate:**
```bash
openspec validate <descriptive-name>
```

### Approach B: Loosely Coupled (Multiple Proposals)

Use this for: Independent subtasks that can parallelize

**For EACH subtask:**
```bash
/openspec:proposal <subtask-specific-name>
```

**Proposal Structure:**
```markdown
# [Subtask-Specific Name]

## Motivation
[Subtask purpose only]

## Scope
Related TaskMaster Task: #[X.Y] only

## Requirements
[Focused on this ONE subtask]

### Requirement: [Subtask Intent]
The system SHALL [behavior]...

**Scenarios:**
[Test scenarios for this subtask only]

## Test Strategy
[Subtask-specific testing with same TDD requirements as Approach A]
```

**Validate:**
```bash
openspec validate <subtask-specific-name>
```

### Update Integration Map

After creating proposals for each batch, append to `TASKMASTER_OPENSPEC_MAP.md`:

```markdown
## Batch [N] - Tasks [start]-[end]

### Tightly Coupled
- TM #3 (subtasks 3.1-3.4): User Authentication -> OpenSpec: user-authentication

### Loosely Coupled
- TM #5.1: Refactor users endpoint -> OpenSpec: refactor-users-endpoint
- TM #5.2: Refactor products endpoint -> OpenSpec: refactor-products-endpoint
- TM #5.3: Refactor orders endpoint -> OpenSpec: refactor-orders-endpoint
```

### Batch Completion Checklist

Before moving to next batch:
- [ ] All proposals created
- [ ] All proposals validated (`openspec validate <name>`)
- [ ] Integration map updated
- [ ] Cross-references accurate
- [ ] Scenarios cover all acceptance criteria

---

## Phase 3: Implementation (CLAUDE CODE)

**Objective:** Implement features according to OpenSpec proposals using Test-Driven Development (TDD), update TaskMaster status.

### Choose Implementation Strategy

```
Decision Tree:
1. Loosely coupled subtasks + want speed? -> Use Strategy 3C (Parallel)
2. Tightly coupled or sequential dependencies? -> Use Strategy 3A (Sequential - Task)
3. Loosely coupled but prefer sequential? -> Use Strategy 3B (Sequential - Subtask)
```

### Strategy 3A: Sequential Implementation - Tightly Coupled

**Use when:** One OpenSpec proposal covers entire task with multiple requirements

```bash
# 1. Load context
task-master show <id>
openspec show <change-name>
# Review: docs/architecture.md (relevant sections)
# Check: TASKMASTER_OPENSPEC_MAP.md for dependencies

# 2. Implement using TEST-DRIVEN DEVELOPMENT (TDD)
/openspec:apply <change-name>

# For each requirement (mapped to subtask if applicable):
#   
#   === TEST-FIRST APPROACH (MANDATORY) ===
#   
#   a. Write failing tests FIRST:
#      - Create test file: tests/<feature>_test.<ext>
#      - Write unit tests for all scenarios
#      - Write integration tests for interactions
#      - Write edge case and error tests
#      - Run tests: npm test (or pytest, etc.)
#      - VERIFY: All new tests FAIL (red)
#   
#   b. Implement minimum code to pass tests:
#      - Write production code
#      - Run tests continuously
#      - VERIFY: All tests PASS (green)
#   
#   c. Refactor while maintaining green tests:
#      - Improve code quality
#      - Run tests after each change
#      - VERIFY: Tests still pass
#   
#   d. Validate test coverage:
npm run coverage
#      - VERIFY: >= 80% line coverage
#      - VERIFY: >= 70% branch coverage
#      - If insufficient: WRITE MORE TESTS
#   
#   e. Run integration tests:
npm test:integration
#      - VERIFY: Component interactions work
#      - VERIFY: No side effects
#   
#   f. Mark subtask complete ONLY if all gates pass:
#      [x] All tests pass
#      [x] Coverage thresholds met
#      [x] No linting errors
#      [x] Integration tests pass
task-master set-task-status --id=<X.Y> --status=done
#   
#   g. Move to next requirement

# 3. When all requirements complete - FULL VALIDATION
npm test
npm test:integration
npm test:e2e  # If applicable
npm run coverage  # MUST show >= 80% line, >= 70% branch
npm test:regression  # Verify no existing features broken
npm run lint
npm run type-check  # If applicable

# ONLY proceed if ALL gates pass:
task-master set-task-status --id=<id> --status=done
openspec archive <change-name> --yes

# 4. Confirm completion
task-master show <id>  # Should show: done
openspec show <change-name>  # Should show: archived
npm run coverage  # Verify coverage maintained
```

**Estimated time:** 90-120 minutes per task (includes comprehensive testing)

### Strategy 3B: Sequential Implementation - Loosely Coupled

**Use when:** Each subtask has own proposal, implementing one at a time

```bash
# For each subtask X.Y:

# 1. Load context
task-master show <X.Y>
openspec show <subtask-change-name>

# 2. Implement using TDD
/openspec:apply <subtask-change-name>
#
# === TEST-FIRST FOR THIS SUBTASK ===
# [Same TDD process as 3A, but focused on single subtask]

# 3. Complete subtask ONLY if gates pass
task-master set-task-status --id=<X.Y> --status=done
openspec archive <subtask-change-name> --yes

# 4. Check parent progress
task-master show <X>
# If all subtasks done:
task-master set-task-status --id=<X> --status=done
# Otherwise: Move to next subtask
```

**Estimated time:** 40-50 minutes per subtask (includes testing)

### Strategy 3C: Parallel Implementation - Loosely Coupled

**Use when:** Multiple independent subtasks can run simultaneously

```bash
# Setup (Main Project):

# 1. Verify independence
task-master show <X>
# Confirm: No dependencies between subtasks

# 2. Create git worktrees (one per subtask)
git worktree add ../project-task-<X.1> -b feature/task-<X.1>-<name>
git worktree add ../project-task-<X.2> -b feature/task-<X.2>-<name>
git worktree add ../project-task-<X.3> -b feature/task-<X.3>-<name>

# 3. Copy context to each worktree
for dir in ../project-task-<X>.*; do
  cp -r .taskmaster "$dir/"
  cp -r openspec "$dir/"
  cp -r docs "$dir/"
  cp TASKMASTER_OPENSPEC_MAP.md "$dir/"
done
```

**Launch Instructions (Per Worktree):**

Create separate Claude Code instance for each with TDD mandate:

```
WORKTREE: ../project-task-<X.Y>

Implement TaskMaster subtask <X.Y> using TEST-DRIVEN DEVELOPMENT:

Context:
- Run: task-master show <X.Y>
- Run: openspec show <subtask-change-name>

Implementation (TDD MANDATORY):
[Same TDD process as strategies above]

Completion:
- task-master set-task-status --id=<X.Y> --status=done
- openspec archive <subtask-change-name> --yes
- Commit: git commit -m "Complete subtask <X.Y> with tests"
```

**Time savings:** 3-4x faster for loosely coupled tasks!

---

## Phase 4: Component Integration Testing (CLAUDE CODE)

**Objective:** Execute Task #24 - Test all integration points between components.

### Integration & System Testing Tasks

**Critical:** These tasks (typically #24-26) are mandatory and must be completed before production deployment.

### Task #24: Component Integration Testing

**Purpose:** Test all integration points between components

**Scope:** 
- Verify data flows correctly between components
- Test error handling between components
- Validate API contracts
- Test database connections and transactions
- Verify external service integrations

**Dependencies:** ALL feature implementation tasks must be complete

**Implementation:**

```bash
# 1. Load task context
task-master show 24
openspec show component-integration-testing

# 2. Execute integration test suite
npm test:integration

# Test categories:
# - Component A <-> Component B interactions
# - Database connection pooling
# - API endpoint integrations
# - External service calls (with test credentials)
# - Error propagation and handling

# 3. Validate coverage
npm run coverage:integration

# Required:
# - All integration points tested: 100%
# - Error scenarios covered: 100%
# - Performance benchmarks met

# 4. Mark complete only if all gates pass
task-master set-task-status --id=24 --status=done
```

**Success Criteria:**
- All integration tests passing (100%)
- All integration points tested
- Error scenarios handled correctly
- Performance benchmarks met
- No errors in service logs

---

## Phase 5: E2E & Production Validation (CLAUDE CODE)

### Task #25: End-to-End Workflow Testing

**Purpose:** Test complete user journeys from start to finish

**Scope:**
- Verify system works from user perspective
- Test critical business processes
- Validate complete workflows
- Test in production-like environment

**Dependencies:** Task #24 (component integration)

**Implementation:**

```bash
# 1. Load task context
task-master show 25
openspec show e2e-workflow-testing

# 2. Execute E2E test suite
npm test:e2e

# Test categories:
# - User registration -> email verification -> first login
# - Browse -> add to cart -> checkout -> payment -> confirmation
# - Account management workflows
# - Error recovery scenarios

# 3. Validate in multiple environments
# - Desktop browsers (Chrome, Firefox, Safari)
# - Mobile viewports (iOS, Android)

# 4. Review test recordings
# - Videos of test execution
# - Screenshots of any failures

# 5. Mark complete only if all gates pass
task-master set-task-status --id=25 --status=done
```

**Success Criteria:**
- All E2E tests passing (100%)
- All critical user journeys validated
- Tests passing in all browsers
- Tests passing in mobile viewports
- No critical UX issues found

### Task #26: Production Readiness Validation

**Purpose:** Complete production readiness checklist and obtain stakeholder sign-offs

**Scope:**
- Final validation before deployment
- Performance, security, operational readiness
- Complete documentation review
- Obtain all required sign-offs

**Dependencies:** Task #25 (E2E workflows)

**Implementation:**

```bash
# 1. Load task context
task-master show 26

# 2. Execute comprehensive validation
#
# === MANDATORY PRODUCTION READINESS GATES ===
#
# Testing Validation:
npm test                    # All unit tests
npm test:integration        # All integration tests
npm test:e2e               # All E2E tests
npm test:regression        # Regression suite
npm run coverage           # >= 80% line, >= 70% branch

# Security Validation:
npm audit                  # No critical vulnerabilities
npm run security:scan      # SAST scan passing

# Performance Validation:
npm test:performance       # Load tests passing
# Verify: Response times < targets
# Verify: Throughput meets requirements

# Operational Validation:
# [ ] Deploy to staging successful
# [ ] All services healthy
# [ ] Monitoring dashboards validated
# [ ] Test alerts firing correctly
# [ ] Logging aggregation working
# [ ] Rollback tested successfully
# [ ] Database migrations tested (up and down)

# Documentation Validation:
# [ ] API documentation complete
# [ ] README updated
# [ ] Architecture docs current
# [ ] Runbook created and reviewed

# 3. Obtain stakeholder sign-offs
# [ ] QA team sign-off
# [ ] Product owner sign-off
# [ ] Security team sign-off (if required)
# [ ] Operations team sign-off (if required)

# 4. Final Go/No-Go decision
# DECISION: [GO / NO-GO]

# 5. Mark complete only if GO decision
task-master set-task-status --id=26 --status=done
```

**Success Criteria:**
- All validation gates passed
- All stakeholder sign-offs obtained
- Production readiness checklist 100% complete
- GO decision documented

**Output:** Go/No-Go decision with complete documentation

---

## Phase 6: Deployment

**Objective:** Deploy validated code to production.

### Deployment Strategy

**Staging Deployment:**
```bash
# 1. Deploy to staging
./scripts/deploy-staging.sh

# 2. Run smoke tests
npm test:smoke

# 3. Manual validation
# - Critical user journeys
# - Performance monitoring
# - Error logs

# Duration: 24-48 hours of observation
```

**Canary Deployment (if applicable):**
```bash
# 1. Deploy to 5% of users
./scripts/deploy-canary.sh

# 2. Monitor metrics
# - Error rates
# - Performance metrics
# - User feedback

# 3. Rollback trigger
# If error rate > 0.1%: ROLLBACK
# Otherwise: Continue to full deployment

# Duration: 24-48 hours
```

**Production Deployment:**
```bash
# 1. Deploy to production
./scripts/deploy-production.sh

# 2. Verify deployment
# - All services healthy
# - Metrics populating correctly
# - No errors in logs

# 3. Intensive monitoring
# Duration: 48-72 hours
```

### Validation Checklist

**Code Integration:**
- [ ] All feature branches merged
- [ ] No merge conflicts

**Test Validation (MANDATORY):**
- [ ] All unit tests passing (100%)
- [ ] All integration tests passing (100%)
- [ ] All E2E tests passing (100%)
- [ ] All regression tests passing (100%)
- [ ] Line coverage >= 80%
- [ ] Branch coverage >= 70%
- [ ] Critical paths 100% covered

**Quality Gates:**
- [ ] No linting errors
- [ ] No type errors
- [ ] No security vulnerabilities
- [ ] Performance benchmarks met

**System Validation:**
- [ ] TaskMaster shows all tasks done
- [ ] OpenSpec shows no active changes
- [ ] Integration map matches reality

**Documentation:**
- [ ] Test reports generated
- [ ] Coverage reports generated
- [ ] API documentation updated

---

## Testing Strategy: Comprehensive Quality Assurance

This workflow mandates Test-Driven Development (TDD) as the ONLY acceptable approach to ensure bug-free, production-quality code.

### Testing Pyramid

All tasks must implement tests across three levels:

```
        /\
       /E2E\        (10% of tests) - End-to-end user workflows
      /------\
     /  INT   \     (30% of tests) - Component integration
    /----------\
   /   UNIT     \   (60% of tests) - Function/method level
  /--------------\
```

**Distribution Guidelines:**
- **Unit Tests (60%):** Test individual functions, methods, classes in isolation
- **Integration Tests (30%):** Test how components work together
- **E2E Tests (10%):** Test complete user workflows (if applicable)

### Test-First Development Process (RED-GREEN-REFACTOR)

**MANDATORY for every subtask:**

```
1. RED: Write failing tests
   - Define test cases from OpenSpec scenarios
   - Write test code BEFORE implementation code
   - Run tests: MUST fail (if they pass, your test is wrong)
   - Commit: "Add failing tests for [feature]"

2. GREEN: Make tests pass
   - Write MINIMUM code to pass tests
   - Run tests continuously
   - Stop when all tests pass
   - Commit: "Implement [feature]"

3. REFACTOR: Improve code quality
   - Clean up code
   - Remove duplication
   - Run tests after EVERY change
   - Tests MUST stay green
   - Commit: "Refactor [feature]"
```

### Coverage Requirements

**Minimum Thresholds (BLOCKING):**
```javascript
// Example: package.json or jest.config.js
{
  "jest": {
    "coverageThreshold": {
      "global": {
        "lines": 80,
        "branches": 70,
        "functions": 80,
        "statements": 80
      },
      "./src/critical/": {
        "lines": 100,
        "branches": 100
      }
    }
  }
}
```

**Critical Paths:** 100% coverage required
- Authentication/authorization
- Payment processing
- Data persistence
- Security-sensitive operations

**Non-Critical Paths:** 80% coverage minimum
- UI components
- Utility functions
- Configuration code

---

## Quick Reference

### TaskMaster Commands

```bash
# View
task-master show <id>              # Single task
task-master show <id>.<subtask>    # Subtask
task-master list                   # All tasks

# Update
task-master set-task-status --id=<id> --status=done

# Analysis (Phase 1)
task-master analyze-complexity --research
task-master complexity-report
task-master expand --id=<X> --research
```

### OpenSpec Commands

```bash
# Create & View
/openspec:proposal <name>          # Create proposal
openspec show <name>               # View proposal
openspec list                      # List all

# Implement & Archive
/openspec:apply <name>             # Implement
openspec validate <name>           # Validate
openspec archive <name> --yes      # Archive
```

### Testing Commands

```bash
# Development
npm test                           # Run all tests
npm test -- --watch               # Watch mode
npm run coverage                   # Coverage report

# Validation
npm test:unit                     # Unit tests only
npm test:integration              # Integration tests
npm test:e2e                      # E2E tests
npm test:regression               # Regression suite
```

---

## Expected Results

### Quality Metrics

- [x] **95% reduction in production bugs** (TDD + TaskMaster's focused scoping)
- [x] **100% test coverage of critical paths** (Mandatory TDD workflow)
- [x] **Complete traceability** (PRD -> Tasks -> Specs -> Tests -> Code)
- [x] **Zero integration failures** (Explicit integration testing phase)
- [x] **Regression-proof** (Comprehensive test suite catches breaking changes)

### Time Investment

**Note:** Times include comprehensive testing (TDD adds 30-40% to implementation time but eliminates debugging time and production bugs)

| Phase | Small (<10) | Medium (10-30) | Large (30+) |
|-------|------------|----------------|-------------|
| Phase 0 | 2-3 hours | 4-6 hours | 1-2 days |
| Phase 1 | 5-10 min | 10-15 min | 15-20 min |
| Phase 2 | 30-45 min | 1.5-2.5 hours | 4-6 hours |
| Phase 3 | 2-3 hours | 6-12 hours | 2-3 weeks |
| Phase 4-5 | 1-2 hours | 3-4 hours | 1-2 days |
| Phase 6 | 1-2 hours | 4-6 hours | 1-2 days |
| **Total** | 1 day | 3-5 days | 3-5 weeks |

**ROI:** The additional 30-40% time investment in TDD:
- Eliminates 80-90% of debugging time
- Prevents 95% of production bugs
- Enables confident refactoring
- Reduces maintenance costs by 60%
- **Net Result: 2-3x faster overall project completion**

---

## Critical Success Factors

1. **Use PRD Template** - Purpose-built for TaskMaster with integration requirements
2. **Claude Projects Generates Tasks** - Strategic task creation with production-grade criteria
3. **Human Reviews Tasks** - Validates quality before automation takes over (MANDATORY)
4. **TaskMaster Analyzes Complexity** - Research-backed subtask generation
5. **Always Write Tests First** - TDD is mandatory, not optional
6. **Coupling Analysis Determines Speed** - Correct identification enables 3-4x speedup
7. **Update Both Systems** - TaskMaster AND OpenSpec must stay in sync
8. **Maintain Traceability** - TASKMASTER_OPENSPEC_MAP.md is source of truth
9. **Never Merge Without Passing Tests** - All validation gates must be green
10. **Integration Testing is Sacred** - Tasks #24-26 are non-negotiable

---

**Remember:** This workflow makes the bad path (write code -> maybe test -> ship bugs) structurally impossible. Trust the process and follow each phase precisely.